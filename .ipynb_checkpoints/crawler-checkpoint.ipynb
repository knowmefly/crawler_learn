{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://weixin.sogou.com/weixin?type=2&query=%E6%89%8B%E6%9C%BA&page=1\n",
      "http://weixin.sogou.com/weixin?type=2&query=%E6%89%8B%E6%9C%BA&page=2\n",
      "共找到2页\n",
      " 第 0 个网页 第0次处理\n",
      " 第 0 个网页 第1次处理\n",
      " 第 0 个网页 第2次处理\n",
      " 第 0 个网页 第3次处理\n",
      " 第 0 个网页 第4次处理\n",
      " 第 0 个网页 第5次处理\n",
      " 第 0 个网页 第6次处理\n",
      " 第 0 个网页 第7次处理\n",
      " 第 0 个网页 第8次处理\n",
      " 第 0 个网页 第9次处理\n",
      " 第 1 个网页 第0次处理\n",
      " 第 1 个网页 第1次处理\n",
      " 第 1 个网页 第2次处理\n",
      " 第 1 个网页 第3次处理\n",
      " 第 1 个网页 第4次处理\n",
      " 第 1 个网页 第5次处理\n",
      " 第 1 个网页 第6次处理\n",
      " 第 1 个网页 第7次处理\n",
      " 第 1 个网页 第8次处理\n",
      " 第 1 个网页 第9次处理\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as request\n",
    "import re\n",
    "import time \n",
    "import urllib.error as error\n",
    "\n",
    "#模拟浏览器\n",
    "headers = ('User-Agent','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/68.0.3440.106 Chrome/68.0.3440.106 Safari/537.36')\n",
    "opener = request.build_opener()\n",
    "opener.addheaders = [headers]\n",
    "#将openern安装为全局\n",
    "request.install_opener(opener)\n",
    "listurl = []\n",
    "#使用代理服务器\n",
    "def use_proxy(proxy_addr,url):\n",
    "    #异常处理\n",
    "    try:\n",
    "        proxy = request.ProxyHandler({'http':proxy_addr})\n",
    "        opener = request.build_opener()\n",
    "        request.install_opener(opener)\n",
    "        data = request.urlopen(url).read().decode('utf-8')\n",
    "        return data\n",
    "    except error.URLError as e:\n",
    "        if hasattr(e, \"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e,\"reason\"):\n",
    "            print(e.reason)\n",
    "        time.sleep(10)\n",
    "    except Exception as e:\n",
    "        print(\"ecception:\"+str(e))\n",
    "        time.sleep(1)\n",
    "#获取所有文章链接\n",
    "def getlisturl(key,pagestart,pageend,proxy):\n",
    "    try:\n",
    "        page = pagestart\n",
    "        keycode = request.quote(key)\n",
    "        for page in range(pagestart, pageend+1):\n",
    "            #构架各页url\n",
    "            url = \"http://weixin.sogou.com/weixin?type=2&query=\"+keycode+\"&page=\" +str(page)\n",
    "            print(url)\n",
    "            data1 = use_proxy(proxy, url)\n",
    "            #获取文章链接正则表达式\n",
    "            listurlpat = '<div class=\"img-box\".*?(http://.*?)\"'\n",
    "            #将每个链接加入listurl\n",
    "            listurl.append(re.compile(listurlpat, re.S).findall(data1))\n",
    "        print(\"共找到\"+str(len(listurl))+\"页\")\n",
    "        return listurl\n",
    "    except error.URLError as e:\n",
    "        if hasattr(e,\"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e,\"reason\"):\n",
    "            print(e.reason)\n",
    "        time.sleep(10)\n",
    "    except Exception as e:\n",
    "        print(\"exception:\"+str(e))\n",
    "        time.sleep(1)\n",
    "\n",
    "#通过链接获取相关内容\n",
    "def getcontent(listurl,proxy):\n",
    "    i = 0\n",
    "    html1 = '''<html>\n",
    "    <head>\n",
    "    <title>微信文章</title>\n",
    "    </head>\n",
    "<body>\n",
    "'''\n",
    "    fh = open(\"myweb/5.html\", \"wb\")\n",
    "    fh.write(html1.encode('utf-8'))\n",
    "    fh.close()\n",
    "    fh = open(\"myweb/5.html\", \"ab\")\n",
    "    for i in range(0,len(listurl)):\n",
    "        for j in range(0,len(listurl[i])):\n",
    "            try:\n",
    "                url = listurl[i][j]\n",
    "                url = url.replace(\"amp;\",\"\")\n",
    "                data = use_proxy(proxy, url)\n",
    "                #文章标题正则\n",
    "                titlepat = '<title>(.*?)</title>'\n",
    "                #文章内容正则\n",
    "                contentpat = 'id=\"js_content\">(.*?)id=\"js_sg_bar\"'\n",
    "                title = re.compile(titlepat, re.S).findall(data)\n",
    "                content = re.compile(contentpat, re.S).findall(data)\n",
    "                #初始化标题和内容\n",
    "                thistitle = \" 此次没有获取到 \"\n",
    "                thiscontene = \" 内容没有找到 \"\n",
    "                if(title!= []):\n",
    "                    thistitle = title[0]\n",
    "                if(content!= []):\n",
    "                    thiscontene = content[0]\n",
    "                #内容合并\n",
    "                dataall = \"<p>标题为\" + thistitle + \"</p><p>内容为：\" + thiscontene + \"</p><br>\"\n",
    "                #内容写入\n",
    "                fh.write(dataall.encode('utf-8'))\n",
    "                print(\" 第 \"+str(i)+\" 个网页 第\" +str(j)+ \"次处理\")\n",
    "            except error.URLError as e:\n",
    "                if hasattr(e,\"code\"):\n",
    "                    print(e.code)\n",
    "                if hasattr(e,\"reason\"):\n",
    "                    print(e.reason)\n",
    "                time.sleep(10)\n",
    "            except Exception as e:\n",
    "                print(\"exception:\"+str(e))\n",
    "                time.sleep(1)\n",
    "    fh.close()\n",
    "    html2 = '''</body>\n",
    "    </html>\n",
    "    '''\n",
    "    fh = open(\"myweb/5.html\", \"ab\")\n",
    "    fh.write(html2.encode('utf-8'))\n",
    "    fh.close()\n",
    "#设置关键词\n",
    "key = \"快递\"\n",
    "proxy = \"222.221.11.119:3128\"\n",
    "proxy2= \"\"\n",
    "pagestart = 1\n",
    "pageend = 2\n",
    "listurl = getlisturl(key,pagestart,pageend,proxy)\n",
    "getcontent(listurl,proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "keywd = \"中国\"\n",
    "url = \"http://www.baidu.com/s?wd=\" \n",
    "key_code = urllib.request.quote(keywd)\n",
    "url = url + key_code\n",
    "req = urllib.request.Request(url)\n",
    "data= urllib.request.urlopen(req).read()\n",
    "fhandle = open(\"myweb/1.html\", \"wb\")\n",
    "fhandle.write(data)\n",
    "fhandle.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "url = \"http://www.iqianyue.com/mypost\"\n",
    "postdata = urllib.parse.urlencode({\"name\":\"123\", \"pass\":\"324\"}).encode('utf-8')\n",
    "req = urllib.request.Request(url,postdata)\n",
    "req.add_header('User-Agent','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/68.0.3440.106 Chrome/68.0.3440.106 Safari/537.36')\n",
    "data = urllib.request.urlopen(req).read()\n",
    "fhandle = open(\"myweb/2.html\", \"wb\")\n",
    "fhandle.write(data)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117804\n"
     ]
    }
   ],
   "source": [
    "def use_proxy(proxy_addr,url):\n",
    "    import urllib.request\n",
    "    proxy = urllib.request.ProxyHandler({'http':proxy_addr})\n",
    "    opener = urllib.request.build_opener(proxy, urllib.request.HTTPHandler)\n",
    "    urllib.request.install_opener(opener)\n",
    "    data = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    return data\n",
    "\n",
    "proxy_addr = \"116.62.194.248:3128\"\n",
    "data = use_proxy(proxy_addr, \"http://www.baidu.com\")\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send: b'GET / HTTP/1.1\\r\\nAccept-Encoding: identity\\r\\nHost: blog.csdn.net\\r\\nConnection: close\\r\\nUser-Agent: Python-urllib/3.5\\r\\n\\r\\n'\n",
      "reply: 'HTTP/1.1 200 OK\\r\\n'\n",
      "header: Server header: Date header: Content-Type header: Transfer-Encoding header: Connection header: Vary header: Set-Cookie header: Set-Cookie header: Vary header: Set-Cookie header: Strict-Transport-Security "
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "httphd = urllib.request.HTTPHandler(debuglevel = 1)\n",
    "httpshd = urllib.request.HTTPSHandler(debuglevel = 1)\n",
    "opener = urllib.request.build_opener(httphd, httpshd)\n",
    "urllib.request.install_opener(opener)\n",
    "data = urllib.request.urlopen(\"https://blog.csdn.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send: b'GET /u012373815/article/details/79221 HTTP/1.1\\r\\nAccept-Encoding: identity\\r\\nHost: blog.csdn.net\\r\\nConnection: close\\r\\nUser-Agent: Python-urllib/3.5\\r\\n\\r\\n'\n",
      "reply: 'HTTP/1.1 404 Not Found\\r\\n'\n",
      "header: Server header: Date header: Content-Type header: Content-Length header: Connection header: Vary header: Set-Cookie header: Set-Cookie header: ETag 404\n",
      "Not Found\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as request\n",
    "import urllib.error as error\n",
    "try:\n",
    "    request.urlopen(\"https://blog.csdn.net/u012373815/article/details/79221\")\n",
    "except error.URLError as e:\n",
    "    print(e.code)\n",
    "    print(e.reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 110] Connection timed out\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as request\n",
    "import urllib.error as error\n",
    "try:\n",
    "    request.urlopen(\"http://10.1.5.214/\")\n",
    "except error.URLError as e:\n",
    "    if hasattr(e,\"code\"):\n",
    "        print(e.code)\n",
    "    if hasattr(e,\"reason\"):\n",
    "        print(e.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(3, 6), match='aaa'>\n",
      "None\n",
      "<_sre.SRE_Match object; span=(7, 12), match='2@qq.'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = \"aaa\"\n",
    "string = \"bbbaaaccc\"\n",
    "result1= re.search(pattern, string)\n",
    "print(result1)\n",
    "pattern = \"\\n\"\n",
    "string = '''abdjas'''\n",
    "result2 = re.search(pattern,string)\n",
    "print(result2)\n",
    "pattern = \"\\d\\Wqq\\W\"\n",
    "string = '''abdjas12@qq.dasdsadsa'''\n",
    "result3 = re.search(pattern,string)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.match()     #起始位置匹配\n",
    "re.search()    #全局检索\n",
    "re.compile() #全局匹配\n",
    "re.sub()          #替换(默认所有)\n",
    "\n",
    "pattern = \"[a-zA-Z]+://[^\\s]*[.com|.cn]\"  #网址匹配原子\n",
    "pattern = \"\\w+([.+-]\\w+)*@\\w+([.-]\\w+)*.\\w+([.-]\\w+)*\" #邮箱匹配原子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as request\n",
    "import urllib.parse as parse\n",
    "import urllib.error as error\n",
    "import http.cookiejar as cookiejar\n",
    "\n",
    "url = \"http://123.207.156.248/wp-login.php\"\n",
    "postdata = parse.urlencode({\n",
    "    \"log\":\"1351393068@qq.com\",\n",
    "    \"pwd\":\"1998.624.dzf\"\n",
    "}).encode('utf-8')\n",
    "req = request.Request(url,postdata)\n",
    "req.add_header('User-Agent','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/68.0.3440.106 Chrome/68.0.3440.106 Safari/537.36')\n",
    "cjar = cookiejar.CookieJar()\n",
    "opener = request.build_opener(request.HTTPCookieProcessor(cjar))\n",
    "request.install_opener(opener)\n",
    "status = opener.open(req).status\n",
    "data = opener.open(req).read()\n",
    "print(status)\n",
    "fhandle = open(\"myweb/3.html\",\"wb\")\n",
    "fhandle.write(data)\n",
    "fhandle.close()\n",
    "url2 = \"http://123.207.156.248/wp-admin/edit.php?post_type=post\"\n",
    "data2 = request.urlopen(url2).read()\n",
    "fhandle = open(\"myweb/4.html\",\"wb\")\n",
    "fhandle.write(data2)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request as request\n",
    "import time\n",
    "\n",
    "def craw(url,page):\n",
    "    html1 = request.urlopen(url).read()\n",
    "    html1 = str(html1)\n",
    "    pat1 = '<div class=\"f-store\".+? <div class=\"page clearfix\">'\n",
    "    result1= re.compile(pat1).findall(html1)\n",
    "    result1 = result1[0]\n",
    "    pat2 = 'src=\"//(.+?\\.jpg)\"'\n",
    "    imagelist = re.compile(pat2).findall(result1)\n",
    "    x = 1\n",
    "    for imageurl in imagelist:\n",
    "        imagename = \"myweb/pic/\"+str(page)+str(x)+\".jpg\"\n",
    "        imageurl = \"http://\"+imageurl\n",
    "        try:\n",
    "            request.urlretrieve(imageurl, filename = imagename)\n",
    "        except error.URLError as e:\n",
    "            if hasattr(e, \"code\"):\n",
    "                x+=1\n",
    "            if hasattr(e, \"reason\"):\n",
    "                x+=1\n",
    "        x+=1\n",
    "    time.sleep(1)\n",
    "for i in range(1,30):\n",
    "    url = \"https://list.jd.com/list.html?cat=9987,653,655&page=\"+str(i)\n",
    "    craw(url,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.request as request\n",
    "def getlink(url):\n",
    "    #模拟浏览器\n",
    "    headers = ('User-Agent','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/68.0.3440.106 Chrome/68.0.3440.106 Safari/537.36')\n",
    "    opener = request.build_opener()\n",
    "    opener.addheaders = [headers]\n",
    "    #将openern安装为全局\n",
    "    request.install_opener(opener)\n",
    "    file = request.urlopen(url)\n",
    "    data = str(file.read())\n",
    "    pat = '(https?://[^\\s)\";]+\\.(\\w|/)*)'\n",
    "    link = re.compile(pat).findall(data)\n",
    "    link = list(set(link))\n",
    "    return link\n",
    "url = \"https://www.csdn.net/\"\n",
    "linklist = getlink(url)\n",
    "print(len(linklist))\n",
    "for link in linklist:\n",
    "    print(link[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "鱼歌浅唱\n",
      "\n",
      "> <span>   得知某连锁药房当天晚上六点在广场举办大型抽奖活动，午休特意赶回家取来医保卡，余额“咔咔”划到个位数，一堆常用药、维生素和鱼肝油换来十多张奖券……六点二十终于赶到人山人海的抽奖现场，在伴随着广场舞曲的嘈杂声中仔细搜索主持人声嘶力竭宣读的兑奖码，认真对照着手里的奖券……十分钟后，身边一位好心的大爷终于看不下去了“姑娘，奖券只留副券兑奖，主券要投到票箱里，不过，六点就封箱了……”  </span>  \n",
      "> <span>   老公种的爬藤红玫瑰沿着阳台的墙壁爬去邻居家了。<br/><br/>今天听到邻居大哥站在阳台上跟我老公说：“你家的红杏爬过来了啊。”<br/><br/>大哥你瞎啊？玫瑰！是红玫瑰！！\u0001[狂躁]\u0001[狂躁]\u0001[狂躁]  </span>  \n",
      "> <span>   开始很恐怖，中间很搞笑，结尾很悲伤<br/>说：从前有一只鬼放了个屁，然后死了  </span>  \n",
      "> <span>   小学三年级，有一次记错了开学时间，我爸妈早上还问我：今天上不上学，我以为他们逗我，之后我看了一天哆啦A梦，直到今天我都记得那天晚上有一场多么痛的领悟，领悟完后，还要抽噎着去上英语补习班，之后还要像同学们解释，我白天拉肚子了_(:з」∠)_  </span>  \n",
      "> <span>   我家种有芒果，网上干嘛大家不跟我买呢，完全没有中间商。还可以吃到新鲜的  </span>  \n",
      "> <span>   同事香水被偷了，在公司里特别气愤的抱怨：你就是偷我男人也行啊，非得偷我香水干什么？  </span>  \n",
      "> <span>   糗百的广告。。这。。我。。。  我还年轻啊  </span>  \n",
      "> <span>   中午，在店里，媳妇来替换我回家，我出门没看见电动车，就问媳妇:车呢？<br/>媳妇头也不抬:就在门口呢，你没看见吗？<br/>我又仔细找了找，还是没有，又问。<br/>媳妇出门一看，哈哈笑着说:那不在那吗？<br/>我顺着她的手指一看，车在街对面停着呢！  </span>  \n",
      "> <span>   老婆，我今天晚点回去！<br/>怎么了？你要干嘛？<br/>我要和哥们去网吧玩一会儿！<br/>不行，我怕黑。<br/>没事亲爱的，不要怕黑，黑只是一种颜色而已！<br/>那么你怕绿吗？绿也只是一种颜色而已！<br/>老婆我现在就回去。  </span>  \n",
      "> <span>   人往高处飞，人越长大，追求会越多，都希望去更大更好的城市发展，但做父母的都希望你留在家乡，越近越好，离的太远会牵挂，会担心，如果选择了自己的梦想，就顾不了家，如果选择了家，就无法达到自己想要的，你这时会怎么选择？  </span>  \n",
      "> <span>   初中的时候特别爱抽烟，每次爸妈都能从我身上闻到一股烟味，但是我一直是死不认账，爸妈也无可奈何…<br/>有一次在客厅思考一道奥数题，我竟然习惯性把笔当烟含在嘴里，仰着头，弄出了抽烟的姿势…<br/>突然一双强劲有力的手抓住了我的臂膀，接着我爸对卧室的我妈大喊：孩子他妈，把绳子和棍子拿出来吧，这次可以确定了…  </span>  \n",
      "> <span>   今天堂妹来我家，由于是我们一大家子最小的老九，我叫她九妹。她刚进来一会我闺女就和她说:九姑姑你回家吧!我瞪了闺女眼说到:你九姑才来让她回什么家!谁知道闺女噘着嘴说到:九姑肯定又是来带着我去见准九姑父。每次带我去以后九姑夫就会给我买一堆好吃的。他一样都不给我就把我送回来。我才不要帮她骗好吃的……  </span>  \n",
      "> <span>   我想问你们各位大神，要是现代的女人在古代，被皇上看中了，带回皇宫。然后侍寝的时候一洗脸。会不会被判个欺君之罪？会不会杀头？那是不是就是说，化妆化太浓了，在古代那是会被杀头的！  </span>  \n",
      "> <span>   要量力而行啊。。  </span>  \n",
      "> <span>   这天气合适吃烧烤。。。。有没有药一起的？  </span>  \n",
      "> <span>   小时候都很穷，没有玩具玩就自己动手做，像木枪，大刀什么的，小伙们一起玩，老爸看我做的都比他们做的好，跟老娘商量了好几年才决定让我学木工，就是怕鲁班祖师爷差了个传人的。  </span>  \n",
      "> <span>   老公蠢直男，不让穿短裤短裙露背装，结婚第一年的夏天，实在太热，穿了一次短裤，结果微信转账我，让我去买几件凉快点的裤子穿。哈哈哈，从此只要缺钱花，就穿条短裤在他面前晃两圈，省事。  </span>  \n",
      "> <span>   最近总不想上班，好希望能买中双色球，然后把离职信扔在上司脸上，大声说老子不干了！理想只是理想而已，不说了，我去搬砖了。  </span>  \n",
      "> <span>   我问大爷怎么保存的那么好，大爷说也就才四十多年而已。  </span>  \n",
      "> <span>   我就想问问，这蛋糕上面还插条黄瓜是几个意思？  </span>  \n",
      "> <span>   这几天婆婆的打牌老战友，突然去幼儿园做起了保洁～<br/>我问婆婆:她差钱啊？退休了还去上班？<br/>婆婆:不是啊！是她那宝贝孙子要上幼儿园了，担心他开学这几天会哭，说懒得扒门栏，蹲窗户的偷看……  </span>  \n",
      "> <span>   有一次去日本出差，晚上跟着几个同事由日方代表带着去当地的KTV“见见世面”。<br/>神通广大的“妈妈 桑”看到我，二话不说从后面拉出来个中国男留学生, 叫他陪我。<br/>那男学生比我还害羞，一出来就委屈地说:“我来刷盘子的，今天才第一天上班......”<br/>然后那晚我花了好几百块钱跟他喝着汽水热烈讨论了俩小时新干线和磁悬浮动车原理。  </span>  \n",
      "> <span>   刚和老娘聊天，聊到我是不是亲生的问题上：<br/>我：算了，今天不饿，就不吃完饭了。<br/>老妈：行呀，不吃就不吃呗，还能给家里省钱<br/>我：你就不怕我饿着？<br/>老妈：这有啥怕的？反正你有不是亲生的。<br/>我：对，我是你充话费送的！<br/>老妈：没错，你就是我当年充20块钱话费送的；明天我就去营业厅，把你送回去！<br/>我：哈哈，这都多少年了，你可得退人家不少钱啊！\u0001[嘻嘻]\u0001[嘻嘻]<br/>老妈：哼，就你这熊玩意儿，我就是返给人家20000块钱也得退回去！\u0001[奸笑]\u0001[奸笑]<br/>我…我………\u0001[捂脸]\u0001[捂脸]\u0001[捂脸]\u0001[捂脸]\u0001[捂 … </span>  <span class=\"contentForAll\">查看全文</span>  \n",
      "> <span>   给你个眼神 自己体会  </span>  \n",
      "> <span>   不是糗事，三年的感情终于结束了，可笑我还想给他一个解释的机会，以后不会再联系  </span>  \n"
     ]
    }
   ],
   "source": [
    "import urllib.request as request\n",
    "import re\n",
    "def getcontent(url,page):\n",
    "    #模拟浏览器\n",
    "    headers = ('User-Agent','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/68.0.3440.106 Chrome/68.0.3440.106 Safari/537.36')\n",
    "    opener = request.build_opener()\n",
    "    opener.addheaders = [headers]\n",
    "    #将openern安装为全局\n",
    "    request.install_opener(opener)\n",
    "    data = request.urlopen(url).read().decode('utf-8')\n",
    "    #用户正则表达式\n",
    "    userpat = '<h2>(.*?)</h2>'\n",
    "    #内容正则表达式\n",
    "    contentpat = '<div class=\"content\"(.*?)</div>'\n",
    "    userlist = re.compile(userpat, re.S).findall(data)\n",
    "    contentlist = re.compile(contentpat, re.S).findall(data)\n",
    "    x = 1\n",
    "    for content in contentlist:\n",
    "        content = content.replace(\"\\n\", \" \")\n",
    "        name = \"content\" + str(x)\n",
    "        #字符串为变量名\n",
    "        exec(name+'=content')\n",
    "        x+=1\n",
    "    \n",
    "    y = 1\n",
    "    for user in userlist:\n",
    "        name = \"content\" + str(y)\n",
    "        print(\"用户\" + str(page) + str(y)+ \"是：\"+ user)\n",
    "        print(\"内容是：\")\n",
    "        exec(\"print(\"+name+\")\")\n",
    "        print(\"\\n\")\n",
    "        y+=1\n",
    "    \n",
    "for i in range(1,2):\n",
    "    url = \"https://www.qiushibaike.com/8hr/page/\" + str(i)\n",
    "    getcontent(url, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
