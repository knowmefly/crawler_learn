{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多线程基础\n",
    "import threading\n",
    "import time\n",
    "class A (threading.Thread):\n",
    "    def __init__(self):\n",
    "        #初始化线程\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        #执行程序内容\n",
    "        for i in range(10):\n",
    "            print(\"线程A\")\n",
    "            time.sleep(2)\n",
    "\n",
    "class B (threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "    def run(self):\n",
    "        for i in range(10):\n",
    "            print(\"我是线程B\")\n",
    "            time.sleep(2)\n",
    "#实例化线程A\n",
    "t1 = A()\n",
    "t1.start()\n",
    "#实例化线程B\n",
    "t2 = B()\n",
    "t2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "import re\n",
    "import time \n",
    "import urllib.error as error\n",
    "\n",
    "#模拟浏览器\n",
    "headers = ('User-Agent','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/68.0.3440.106 Chrome/68.0.3440.106 Safari/537.36')\n",
    "opener = request.build_opener()\n",
    "opener.addheaders = [headers]\n",
    "#将openern安装为全局\n",
    "request.install_opener(opener)\n",
    "listurl = []\n",
    "#使用代理服务器\n",
    "def use_proxy(proxy_addr,url):\n",
    "    #异常处理\n",
    "    try:\n",
    "        proxy = request.ProxyHandler({'http':proxy_addr})\n",
    "        opener = request.build_opener(proxy, request.HTTPHandler)\n",
    "        request.install_opener(opener)\n",
    "        data = request.urlopen(url).read().decode('utf-8')\n",
    "        return data\n",
    "    except error.URLError as e:\n",
    "        if hasattr(e, \"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e,\"reason\"):\n",
    "            print(e.reason)\n",
    "        time.sleep(10)\n",
    "    except Exception as e:\n",
    "        print(\"ecception:\"+str(e))\n",
    "        time.sleep(1)\n",
    "#获取所有文章链接\n",
    "def getlisturl(key,pagestart,pageend,proxy):\n",
    "    try:\n",
    "        page = pagestart\n",
    "        keycode = request.quote(key)\n",
    "        pagecode = request.quote(\"&page\")\n",
    "        for page in range(pagestart, pageend+1):\n",
    "            #构架各页url\n",
    "            url = \"http://weixin.sogou.com/weixin?type=2&query=\"+keycode+\"&page=\" +str(page)\n",
    "            print(url)\n",
    "            data1 = use_proxy(proxy, url)\n",
    "            #print(data1)\n",
    "            #获取文章链接正则表达式\n",
    "            listurlpat = '<div class=\"img-box\".*?(http://.*?)\"'\n",
    "            #将每个链接加入listurl\n",
    "            listurl.append(re.compile(listurlpat, re.S).findall(data1))\n",
    "        print(\"共找到\"+str(len(listurl))+\"页\")\n",
    "        print(listurl)\n",
    "        return listurl\n",
    "    except error.URLError as e:\n",
    "        if hasattr(e,\"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e,\"reason\"):\n",
    "            print(e.reason)\n",
    "        time.sleep(10)\n",
    "    except Exception as e:\n",
    "        print(\"exception:\"+str(e))\n",
    "        time.sleep(1)\n",
    "\n",
    "#通过链接获取相关内容\n",
    "def getcontent(listurl,proxy):\n",
    "    i = 0\n",
    "    html1 = '''<html>\n",
    "    <head>\n",
    "    <title>微信文章</title>\n",
    "    </head>\n",
    "<body>\n",
    "'''\n",
    "    fh = open(\"myweb/5.html\", \"wb\")\n",
    "    fh.write(html1.encode('utf-8'))\n",
    "    fh.close()\n",
    "    fh = open(\"myweb/5.html\", \"ab\")\n",
    "    for i in range(0,len(listurl)):\n",
    "        for j in range(0,len(listurl[i])):\n",
    "            try:\n",
    "                url = listurl[i][j]\n",
    "                url = url.replace(\"amp;\",\"\")\n",
    "                data = use_proxy(proxy, url)\n",
    "                #文章标题正则\n",
    "                titlepat = '<title>(.*?)</title>'\n",
    "                #文章内容正则\n",
    "                contentpat = 'id=\"js_content\">(.*?)id=\"js_sg_bar\"'\n",
    "                title = re.compile(titlepat, re.S).findall(data)\n",
    "                content = re.compile(contentpat, re.S).findall(data)\n",
    "                #初始化标题和内容\n",
    "                thistitle = \" 此次没有获取到 \"\n",
    "                thiscontene = \" 内容没有找到 \"\n",
    "                if(title!= []):\n",
    "                    thistitle = title[0]\n",
    "                if(content!= []):\n",
    "                    thiscontene = content[0]\n",
    "                #内容合并\n",
    "                dataall = \"<p>标题为\" + thistitle + \"</p><p>内容为：\" + thiscontene + \"</p><br>\"\n",
    "                #内容写入\n",
    "                fh.write(dataall.encode('utf-8'))\n",
    "                print(\" 第 \"+str(i)+\" 个网页 第\" +str(j)+ \"次处理\")\n",
    "            except error.URLError as e:\n",
    "                if hasattr(e,\"code\"):\n",
    "                    print(e.code)\n",
    "                if hasattr(e,\"reason\"):\n",
    "                    print(e.reason)\n",
    "                time.sleep(10)\n",
    "            except Exception as e:\n",
    "                print(\"exception:\"+str(e))\n",
    "                time.sleep(1)\n",
    "    fh.close()\n",
    "    html2 = '''</body>\n",
    "    </html>\n",
    "    '''\n",
    "    fh = open(\"myweb/5.html\", \"ab\")\n",
    "    fh.write(html2.encode('utf-8'))\n",
    "    fh.close()\n",
    "#设置关键词\n",
    "key = \"快递\"\n",
    "proxy = \"222.221.11.119:3128\"\n",
    "proxy2= \"\"\n",
    "pagestart = 1\n",
    "pageend = 2\n",
    "listurl = getlisturl(key,pagestart,pageend,proxy)\n",
    "getcontent(listurl,proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "keywd = \"中国\"\n",
    "url = \"http://www.baidu.com/s?wd=\" \n",
    "key_code = urllib.request.quote(keywd)\n",
    "url = url + key_code\n",
    "req = urllib.request.Request(url)\n",
    "data= urllib.request.urlopen(req).read()\n",
    "fhandle = open(\"myweb/1.html\", \"wb\")\n",
    "fhandle.write(data)\n",
    "fhandle.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "url = \"http://www.iqianyue.com/mypost\"\n",
    "postdata = urllib.parse.urlencode({\"name\":\"123\", \"pass\":\"324\"}).encode('utf-8')\n",
    "req = urllib.request.Request(url,postdata)\n",
    "req.add_header('User-Agent','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/68.0.3440.106 Chrome/68.0.3440.106 Safari/537.36')\n",
    "data = urllib.request.urlopen(req).read()\n",
    "fhandle = open(\"myweb/2.html\", \"wb\")\n",
    "fhandle.write(data)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_proxy(proxy_addr,url):\n",
    "    import urllib.request\n",
    "    proxy = urllib.request.ProxyHandler({'http':proxy_addr})\n",
    "    opener = urllib.request.build_opener(proxy, urllib.request.HTTPHandler)\n",
    "    urllib.request.install_opener(opener)\n",
    "    data = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    return data\n",
    "\n",
    "proxy_addr = \"163.125.149.93:9999\"\n",
    "data = use_proxy(proxy_addr, \"http://www.baidu.com\")\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "httphd = urllib.request.HTTPHandler(debuglevel = 1)\n",
    "httpshd = urllib.request.HTTPSHandler(debuglevel = 1)\n",
    "opener = urllib.request.build_opener(httphd, httpshd)\n",
    "urllib.request.install_opener(opener)\n",
    "data = urllib.request.urlopen(\"https://blog.csdn.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "import urllib.error as error\n",
    "try:\n",
    "    request.urlopen(\"https://blog.csdn.net/u012373815/article/details/79221\")\n",
    "except error.URLError as e:\n",
    "    print(e.code)\n",
    "    print(e.reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "import urllib.error as error\n",
    "try:\n",
    "    request.urlopen(\"http://10.1.5.214/\")\n",
    "except error.URLError as e:\n",
    "    if hasattr(e,\"code\"):\n",
    "        print(e.code)\n",
    "    if hasattr(e,\"reason\"):\n",
    "        print(e.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = \"aaa\"\n",
    "string = \"bbbaaaccc\"\n",
    "result1= re.search(pattern, string)\n",
    "print(result1)\n",
    "pattern = \"\\n\"\n",
    "string = '''abdjas'''\n",
    "result2 = re.search(pattern,string)\n",
    "print(result2)\n",
    "pattern = \"\\d\\Wqq\\W\"\n",
    "string = '''abdjas12@qq.dasdsadsa'''\n",
    "result3 = re.search(pattern,string)\n",
    "print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.match()     #起始位置匹配\n",
    "re.search()    #全局检索\n",
    "re.compile() #全局匹配\n",
    "re.sub()          #替换(默认所有)\n",
    "\n",
    "pattern = \"[a-zA-Z]+://[^\\s]*[.com|.cn]\"  #网址匹配原子\n",
    "pattern = \"\\w+([.+-]\\w+)*@\\w+([.-]\\w+)*.\\w+([.-]\\w+)*\" #邮箱匹配原子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "import urllib.parse as parse\n",
    "import urllib.error as error\n",
    "import http.cookiejar as cookiejar\n",
    "\n",
    "url = \"http://123.207.156.248/wp-login.php\"\n",
    "postdata = parse.urlencode({\n",
    "    \"log\":\"1351393068@qq.com\",\n",
    "    \"pwd\":\"1998.624.dzf\"\n",
    "}).encode('utf-8')\n",
    "req = request.Request(url,postdata)\n",
    "req.add_header('User-Agent','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/68.0.3440.106 Chrome/68.0.3440.106 Safari/537.36')\n",
    "cjar = cookiejar.CookieJar()\n",
    "opener = request.build_opener(request.HTTPCookieProcessor(cjar))\n",
    "request.install_opener(opener)\n",
    "status = opener.open(req).status\n",
    "data = opener.open(req).read()\n",
    "print(status)\n",
    "fhandle = open(\"myweb/3.html\",\"wb\")\n",
    "fhandle.write(data)\n",
    "fhandle.close()\n",
    "url2 = \"http://123.207.156.248/wp-admin/edit.php?post_type=post\"\n",
    "data2 = request.urlopen(url2).read()\n",
    "fhandle = open(\"myweb/4.html\",\"wb\")\n",
    "fhandle.write(data2)\n",
    "fhandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request as request\n",
    "import time\n",
    "\n",
    "def craw(url,page):\n",
    "    html1 = request.urlopen(url).read()\n",
    "    html1 = str(html1)\n",
    "    pat1 = '<div class=\"f-store\".+? <div class=\"page clearfix\">'\n",
    "    result1= re.compile(pat1).findall(html1)\n",
    "    result1 = result1[0]\n",
    "    pat2 = 'src=\"//(.+?\\.jpg)\"'\n",
    "    imagelist = re.compile(pat2).findall(result1)\n",
    "    x = 1\n",
    "    for imageurl in imagelist:\n",
    "        imagename = \"myweb/pic/\"+str(page)+str(x)+\".jpg\"\n",
    "        imageurl = \"http://\"+imageurl\n",
    "        try:\n",
    "            request.urlretrieve(imageurl, filename = imagename)\n",
    "        except error.URLError as e:\n",
    "            if hasattr(e, \"code\"):\n",
    "                x+=1\n",
    "            if hasattr(e, \"reason\"):\n",
    "                x+=1\n",
    "        x+=1\n",
    "    time.sleep(1)\n",
    "for i in range(1,30):\n",
    "    url = \"https://list.jd.com/list.html?cat=9987,653,655&page=\"+str(i)\n",
    "    craw(url,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request as request\n",
    "def getlink(url):\n",
    "    #模拟浏览器\n",
    "    headers = ('User-Agent','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/68.0.3440.106 Chrome/68.0.3440.106 Safari/537.36')\n",
    "    opener = request.build_opener()\n",
    "    opener.addheaders = [headers]\n",
    "    #将openern安装为全局\n",
    "    request.install_opener(opener)\n",
    "    file = request.urlopen(url)\n",
    "    data = str(file.read())\n",
    "    pat = '(https?://[^\\s)\";]+\\.(\\w|/)*)'\n",
    "    link = re.compile(pat).findall(data)\n",
    "    link = list(set(link))\n",
    "    return link\n",
    "url = \"https://www.csdn.net/\"\n",
    "linklist = getlink(url)\n",
    "print(len(linklist))\n",
    "for link in linklist:\n",
    "    print(link[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户11是：\n",
      "情迷梦中\n",
      "\n",
      "内容是：\n",
      "> <span>   哪个毒鸡汤说的，你看到心仪的女生不去搭讪，等别人搭讪你吗？~~~~~~听说要隔~~~~~~周六下午回程地铁上，突然发现与我一起上来的一个妹子在我身边，越看越好看的感觉，然后内心交战要不要搭讪，胆怯中十几站路半小时已经过去。突然妹子要下了，我心里想下去搭讪，可是没有勇气啊！地铁门关上的瞬间，后悔自责蜂拥而来，思索再三下一站下车，然后还回到上一站，如果这个来回近十分钟还能找到她，也许就是天意，我一定搭讪一次。终于，回到了上一站，她在，她在，她就在下车的地方，我忍不住的欣喜若狂，老天保佑，她上了手扶电 … </span>  <span class=\"contentForAll\">查看全文</span>  \n",
      "\n",
      "\n",
      "用户12是：\n",
      "谁动了我的白菜\n",
      "\n",
      "内容是：\n",
      "> <span>   跟我弟弟聊天，他说:小时候老被你打，我就想着长大了我一定打回来!<br/>我:你现在比我还高一个头，可以打回来了，对了，这个月零花钱够吗？还要买新衣服吗？<br/>我弟:后来我发现我长大了，你开始讲道理了，我还是打不过\u0001[大哭]\u0001[大哭]\u0001[大哭]...  </span>  \n",
      "\n",
      "\n",
      "用户13是：\n",
      "一幅幅IE\n",
      "\n",
      "内容是：\n",
      "> <span>   哈哈哈哈哈哈哈哈  </span>  \n",
      "\n",
      "\n",
      "用户14是：\n",
      "金玉琵琶\n",
      "\n",
      "内容是：\n",
      "> <span>   这件事说起来挺隐私的—<br/><br/>一直发现马桶坐垫圈掀起来时，垫圈反面总挂有尿渍，而且只在最前面的那个角落，液珠还特别大，觉得很诡异，这马桶想来想去也就只有我和老公俩人用，我每天怎么用的自是问心无愧，而他一个男人如厕连垫圈都用不上，要在溅尿在垫圈更是无从谈起。<br/><br/>那会是谁呢？我一直百思不得其解，甚至为此自责不已，觉得一定是自己不小心弄上去的。<br/><br/>直到有一次，我出差数日，回到家时发现垫圈上依然有尿渍，便自言自语地问“怎么还有啊？！”<br/><br/>没想到这时，二货老公羞羞答答地答话了“这个嘛……其实…我每次大号的时候 … </span>  <span class=\"contentForAll\">查看全文</span>  \n",
      "\n",
      "\n",
      "用户15是：\n",
      "很普通的男人\n",
      "\n",
      "内容是：\n",
      "> <span>   刚发生的真事，小姨网购，要求卖家把物品邮寄到黑龙江省虎林市，卖家问有具体地址呢！具体是什么路，小姨淡定的回复水泥路，卖家瞬间惊呆了！！水泥路！！虎林人都实在，家乡人互粉吧！  </span>  \n",
      "\n",
      "\n",
      "用户16是：\n",
      "十里柔情一帘幽梦\n",
      "\n",
      "内容是：\n",
      "> <span>   闺蜜云云正在谈婚论嫁，我一次去医院无意中看到她男朋友挂男科专家号，回来我郑重的约云云出来谈心，劝她为了幸福着想，慎重考虑。<br/><br/>云云娇羞地说：我知道他有问题，但我不介意。你不知道，他不仅仅有钱，而且口才好、动手动力更强！  </span>  \n",
      "\n",
      "\n",
      "用户17是：\n",
      "我是煮茶\n",
      "\n",
      "内容是：\n",
      "> <span>   小时候，我家有个小彩电，乡里乡亲来我家看电视的场面很是壮观，满满一屋子人。     我发小晚来了，看到没位子了，很生气，冲我吼：“叫你留个位子给我和我爸，你没有留！”   我：“这么晚了，你爸呢！？”    我发小：“马上到！”他朝门外喊：“爸，爸...你快来！”  这时他家大黄狗冲了进来……  </span>  \n",
      "\n",
      "\n",
      "用户18是：\n",
      "冰丝冰结\n",
      "\n",
      "内容是：\n",
      "> <span>   甲，看我右手。<br/>乙，哇，肌肉那么多。<br/>甲，哼哼。<br/>乙，你一定单身了很多年吧？<br/>甲，说什么，哥可是有女朋友的人。<br/>乙，那肌肉怎么那么好？<br/>甲，还不是放假写作业给练的。  </span>  \n",
      "\n",
      "\n",
      "用户19是：\n",
      "z&amp;L我们…\n",
      "\n",
      "内容是：\n",
      "> <span>   很久没回家了，今天回家后看见妈妈的头发都花白了，看着忙碌的背影，我的眼眶湿润了……这时妈妈转身问我：昨天刚染的奶奶灰，酷不酷？我……  </span>  \n",
      "\n",
      "\n",
      "用户110是：\n",
      "小陆二八\n",
      "\n",
      "内容是：\n",
      "> <span>   无意间看见老公裤子的拉裢没拉上，就好心好意提醒:&quot;你的天窗开着呢！&quot;然后他答:&quot;怕什么，别人又要不去，怎么，这么关心你的专用品啊！&quot;&quot;呃，是我的，拿过来，别放你那了啦！真是好心不得。&quot;给他个白眼…  </span>  \n",
      "\n",
      "\n",
      "用户111是：\n",
      "清明［上］河图\n",
      "\n",
      "内容是：\n",
      "> <span>   很久很久以前，两个裁缝来见一个国王，他们自称能做出最美的衣服，但是愚蠢的人看不见它。裁缝给国王穿上问美不美，国王明明看不见衣服却也只能硬着说漂亮，第二天，国王宣布，给全国的姑娘每人做一套这样的衣服。  </span>  \n",
      "\n",
      "\n",
      "用户112是：\n",
      "慕容语嫣~\n",
      "\n",
      "内容是：\n",
      "> <span>   闺蜜打电话给我，说做噩梦了，问她什么噩梦？闺蜜说：“梦到有个帅哥拿着刀，闯我房间来了！”  我：“真可怕，然后呢？”  闺蜜接着说：“然后就把屋里值钱的东西都抢走了…”  我安慰：“好了好了，别怕别怕，帅哥只抢东西，而且是做梦。”  闺蜜不悦的回答：“我朦胧记得穿着性感的睡衣，那帅哥却只劫财…”  </span>  \n",
      "\n",
      "\n",
      "用户113是：\n",
      "芭芘红颜\n",
      "\n",
      "内容是：\n",
      "> <span>   二货老公喜欢裸睡，但是又怕闺女看见，所以他总把内裤脱在床边然后盖好被子睡，可是每天早上内裤都在地上，我们都以为是他睡觉不老实，直到我昨天晚睡，亲眼看见我家猫用爪子给他内裤扒拉地上，然后自己在那个位置上睡觉。我才明白，感情这内裤占了猫的地盘了\u0001[惊恐]\u0001[惊恐]\u0001[惊恐]\u0001[惊恐]  </span>  \n",
      "\n",
      "\n",
      "用户114是：\n",
      "幸福+载中……\n",
      "\n",
      "内容是：\n",
      "> <span>   吃鸡随机队友竟然在一栋楼里\u0001[哈哈]  </span>  \n",
      "\n",
      "\n",
      "用户115是：\n",
      "何欣宴\n",
      "\n",
      "内容是：\n",
      "> <span>   结婚之后说只要你生的我都要，晚了，没有  </span>  \n",
      "\n",
      "\n",
      "用户116是：\n",
      "佛系焦糖小饼干～\n",
      "\n",
      "内容是：\n",
      "> <span>   几年前，堂弟第一次相亲，媒人让两人在公园见面，先聊聊。二婶性子急，想看看姑娘长啥样、人品如何。<br/>于是，二婶装成溜弯的大妈，‘不经意’在两人面路过7、8次。她觉得不过瘾，还让二叔穿上破衣服，扮成乞丐，拿个破搪瓷缸子向姑娘讨钱……  </span>  \n",
      "\n",
      "\n",
      "用户117是：\n",
      "王云（笨笨）\n",
      "\n",
      "内容是：\n",
      "> <span>   单位座机在主任座位跟前，我过去用座机打电话，突然打了个喷嚏，刚好我用手捂住了。<br/>主任笑着说：好险，差点打我一脸。<br/>说时迟那时快，我又一个喷嚏，打了主任一脸……<br/>主任保持着那个笑容呆住了……  </span>  \n",
      "\n",
      "\n",
      "用户118是：\n",
      "手机用户24076…\n",
      "\n",
      "内容是：\n",
      "> <span>   我老婆想买个900元的文胸，我说没钱，她就生气了，说我态度不好。我死活想不通哪不好了，一直和颜悦色的啊。  </span>  \n",
      "\n",
      "\n",
      "用户119是：\n",
      "芭芘红颜\n",
      "\n",
      "内容是：\n",
      "> <span>   在商场从厕所出来，一个女的迎上来，热情的说：亲爱的，我和你聊聊减肥你建议吗？<br/>我点点头。<br/>她：你是对我有防备心理还是不想减肥？<br/>我说：都有。<br/>她继续：但是我是很专业的，我可以提供给你健康营养的减肥方法。<br/>我：我老公花200万给我养胖，你居然想让我再花钱减掉？<br/>她：……打扰了！<br/>老公：媳妇儿你这数学真不咋滴，咱俩结婚半年你就花了70万，现在已经结婚六年半了，你好好算算你这肚子值多少钱！<br/>\u0001[哀怨]\u0001[哀怨]\u0001[哀怨]\u0001[哀怨]\u0001[哀怨]  </span>  \n",
      "\n",
      "\n",
      "用户120是：\n",
      "我有蔬菜\n",
      "\n",
      "内容是：\n",
      "> <span>   求稳司机…巨神峰，有没有人带我一下，晋级赛，名字叫策士f。不坑…  </span>  \n",
      "\n",
      "\n",
      "用户121是：\n",
      "哇塞~肉肉\n",
      "\n",
      "内容是：\n",
      "> <span>   家里的长筒保鲜膜没啥用，后来发现可以放在床头，晚上不想起身关灯的时候用来戳开关！<br/>二货闺蜜来我家看到我床头的保鲜膜，问我，床头放这干啥？<br/>我说，这是我的秘密武器，可好用了！<br/>二货白了我一眼说，穷疯了，避孕套能值几个钱？  </span>  \n",
      "\n",
      "\n",
      "用户122是：\n",
      "聊天不撩妹\n",
      "\n",
      "内容是：\n",
      "> <span>   老婆从网上帮儿子买了件衣服  ，儿子说不好看不要，老婆说你穿，我试试还行，就穿上了<br/>昨天在地铁上居然撞杉了，对方是小伙子，边上都在看我们笑，小伙子看了看我说到：大叔你让让，我到另一接车厢去，不能坐个车还认个爹了，怎么看都像父子装了，他一走坐位的姑娘说：长的也真像，我  </span>  \n",
      "\n",
      "\n",
      "用户123是：\n",
      "小明，按住啦掰b\n",
      "\n",
      "内容是：\n",
      "> <span>   就在刚才，打牌把老婆辛苦攒下的120万给输光了。我想了很多，想起我们一点点的攒，想着从无倒有，，，诶！算了，反正明天还能领两回四千。。。  </span>  \n",
      "\n",
      "\n",
      "用户124是：\n",
      "Camillaba…\n",
      "\n",
      "内容是：\n",
      "> <span>   男朋友说题足球可以减肥\u0001[心情不好]  </span>  \n",
      "\n",
      "\n",
      "用户125是：\n",
      "咯去我爱你\n",
      "\n",
      "内容是：\n",
      "> <span>   各位大神能不能帮我想些跟   燕芳   有关的4至6字的词  </span>  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#趣事百科爬取\n",
    "import urllib.request as request\n",
    "import re\n",
    "def getcontent(url,page):\n",
    "    #模拟浏览器\n",
    "    headers = ('User-Agent','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/68.0.3440.106 Chrome/68.0.3440.106 Safari/537.36')\n",
    "    opener = request.build_opener()\n",
    "    opener.addheaders = [headers]\n",
    "    #将openern安装为全局\n",
    "    request.install_opener(opener)\n",
    "    data = request.urlopen(url).read().decode('utf-8')\n",
    "    #用户正则表达式\n",
    "    userpat = '<h2>(.*?)</h2>'\n",
    "    #内容正则表达式\n",
    "    contentpat = '<div class=\"content\"(.*?)</div>'\n",
    "    userlist = re.compile(userpat, re.S).findall(data)\n",
    "    contentlist = re.compile(contentpat, re.S).findall(data)\n",
    "    x = 1\n",
    "    for content in contentlist:\n",
    "        content = content.replace(\"\\n\", \" \")\n",
    "        name = \"content\" + str(x)\n",
    "        #字符串为变量名\n",
    "        exec(name+'=content')\n",
    "        x+=1\n",
    "    \n",
    "    y = 1\n",
    "    for user in userlist:\n",
    "        name = \"content\" + str(y)\n",
    "        print(\"用户\" + str(page) + str(y)+ \"是：\"+ user)\n",
    "        print(\"内容是：\")\n",
    "        exec(\"print(\"+name+\")\")\n",
    "        print(\"\\n\")\n",
    "        y+=1\n",
    "    \n",
    "for i in range(1,2):\n",
    "    url = \"https://www.qiushibaike.com/8hr/page/\" + str(i)\n",
    "    getcontent(url, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
