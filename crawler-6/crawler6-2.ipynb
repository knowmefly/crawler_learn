{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import threading\n",
    "import re\n",
    "import urllib.request as request\n",
    "import time\n",
    "import urllib.error as error\n",
    "\n",
    "urlqueue = queue.Queue()\n",
    "#模拟浏览器\n",
    "headers = ('User-Agent','Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/68.0.3440.106 Chrome/68.0.3440.106 Safari/537.36')\n",
    "opener = request.build_opener()\n",
    "opener.addheaders = [headers]\n",
    "#将openern安装为全局\n",
    "request.install_opener(opener)\n",
    "listurl = []\n",
    "#使用代理服务器\n",
    "def use_proxy(proxy_addr,url):\n",
    "    import urllib.request as request\n",
    "    #异常处理\n",
    "    try:\n",
    "        proxy = request.ProxyHandler({'http':proxy_addr})\n",
    "        opener = request.build_opener(proxy, request.HTTPHandler)\n",
    "        request.install_opener(opener)\n",
    "        data = request.urlopen(url).read().decode('utf-8')\n",
    "        print(data)\n",
    "        print(\"proxy 1\")\n",
    "        return data\n",
    "    except error.URLError as e:\n",
    "        if hasattr(e, \"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e,\"reason\"):\n",
    "            print(e.reason)\n",
    "        time.sleep(10)\n",
    "    except Exception as e:\n",
    "        print(\"ecception:\"+str(e))\n",
    "        time.sleep(1)\n",
    "#线程1，获取对应网站并放入url队列\n",
    "class geturl(threading.Thread):\n",
    "    def __init__(self,key,pagestart,pageend,proxy,urlqueue):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.pagestart = pagestart\n",
    "        self.pageend = pageend\n",
    "        self.proxy = proxy\n",
    "        self.urlqueue=urlqueue\n",
    "        self.key = key\n",
    "    def run(self):\n",
    "        page = self.pagestart\n",
    "        #编码key\n",
    "        keycode = request.quote(key)\n",
    "        #编码page\n",
    "        pagecode = request.quote(\"&page\")\n",
    "        for page in range(pagestart, pageend+1):\n",
    "            #构架各页url\n",
    "            url = \"http://weixin.sogou.com/weixin?type=2&query=\"+keycode+\"&page=\" +str(page)\n",
    "            #print(url)\n",
    "            data1 = use_proxy(proxy, url)\n",
    "            print(proxy)\n",
    "            print(url)\n",
    "            #获取文章链接正则表达式\n",
    "            listurlpat = '<div class=\"img-box\".*?(http://.*?)\"'\n",
    "            #将每个链接加入listurl\n",
    "            listurl.append(re.compile(listurlpat, re.S).findall(data1))\n",
    "        print(\"共找到\"+str(len(listurl))+\"页\")\n",
    "        for i in range(0,len(listurl)):\n",
    "            time.sleep(7)\n",
    "            for j in range(0,len(listurl[i])):\n",
    "                try:\n",
    "                    url = listurl[i][j]\n",
    "                    #处理url多余元素\n",
    "                    url = url.replace(\"amp;\", \"\")\n",
    "                    print(\"第\"+str(i)+\"i\"+str(j)+\"j次入队\")\n",
    "                    self.urlqueue.put(url)\n",
    "                    self.urlqueue.task_done()\n",
    "                except error.URLError as e:\n",
    "                    if hasattr(e,\"code\"):\n",
    "                        print(e.code)\n",
    "                    if hasattr(e,\"reason\"):\n",
    "                        print(e.reason)\n",
    "                    time.sleep(10)\n",
    "                except Exception as e:\n",
    "                    print(\"exception:\"+str(e))\n",
    "                    time.sleep(1)\n",
    "#线程2与线程1并行执行，获取到url并对文章信息处理\n",
    "class getcontent(threading.Thread):\n",
    "    def __init__(self,urlqueue,proxy):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.urlqueue = urlqueue\n",
    "        self.proxy = proxy\n",
    "    def run(self):\n",
    "        html1 = '''<html>\n",
    "    <head>\n",
    "    <title>微信文章</title>\n",
    "    </head>\n",
    "<body>\n",
    "'''\n",
    "        fh = open(\"myweb/6.html\", \"wb\")\n",
    "        fh.write(html1.encode('utf-8'))\n",
    "        fh.close()\n",
    "        fh = open(\"myweb/6.html\", \"ab\")\n",
    "        i = 1\n",
    "        while(True):\n",
    "            try:\n",
    "                url = self.urlqueue.get()\n",
    "                data = use_proxy(proxy, url)\n",
    "                #文章标题正则\n",
    "                titlepat = '<title>(.*?)</title>'\n",
    "                #文章内容正则\n",
    "                contentpat = 'id=\"js_content\">(.*?)id=\"js_sg_bar\"'\n",
    "                title = re.compile(titlepat, re.S).findall(data)\n",
    "                content = re.compile(contentpat, re.S).findall(data)\n",
    "                #初始化标题和内容\n",
    "                thistitle = \" 此次没有获取到 \"\n",
    "                thiscontene = \" 内容没有找到 \"\n",
    "                if(title!= []):\n",
    "                    thistitle = title[0]\n",
    "                if(content!= []):\n",
    "                    thiscontene = content[0]\n",
    "                #内容合并\n",
    "                dataall = \"<p>标题为\" + thistitle + \"</p><p>内容为：\" + thiscontene + \"</p><br>\"\n",
    "                #内容写入\n",
    "                fh.write(dataall.encode('utf-8'))\n",
    "                print(\" 第 \"+str(i)+\" 个网页处理\")\n",
    "                i+=1\n",
    "            except error.URLError as e:\n",
    "                if hasattr(e,\"code\"):\n",
    "                    print(e.code)\n",
    "                if hasattr(e,\"reason\"):\n",
    "                    print(e.reason)\n",
    "                time.sleep(10)\n",
    "            except Exception as e:\n",
    "                print(\"exception:\"+str(e))\n",
    "                time.sleep(1)\n",
    "        fh.close()\n",
    "        html2 = '''</body>\n",
    "        </html>\n",
    "        '''\n",
    "        fh = open(\"myweb/6.html\", \"ab\")\n",
    "        fh.write(html2.encode('utf-8'))\n",
    "        fh.close()\n",
    "class control(threading.Thread):\n",
    "    def __init__(self,urlqueue):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.urlqueue = urlqueue\n",
    "    def run (self):\n",
    "        while(True):\n",
    "            print(\"程序执行中...\")\n",
    "            time.sleep(60)\n",
    "            if(self.urlqueue.empty()):\n",
    "                print(\"程序执行完毕！\")\n",
    "                exit()\n",
    "key = \"人能\"\n",
    "proxy = \"163.125.149.93:9999\"\n",
    "proxy2 = \"61.135.180.27:9000\"\n",
    "#爬取页规定\n",
    "pagestart = 1\n",
    "pageend = 2\n",
    "#创建线程1启动线程1\n",
    "t1 = geturl(key,pagestart,pageend,proxy,urlqueue)\n",
    "t1.start()\n",
    "#创建线程2\n",
    "t2 = getcontent(urlqueue,proxy2)\n",
    "t2.start()\n",
    "#创建线程3并启动\n",
    "t3 = control(urlqueue)\n",
    "t3.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
